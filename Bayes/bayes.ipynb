{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spam and Ham dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spam_path = \"./data/train/spam\"\n",
    "train_ham_path = \"./data/train/ham\"\n",
    "test_spam_path = \"./data/test/spam\"\n",
    "test_ham_path = \"./data/test/ham\"\n",
    "\n",
    "if not os.path.exists(train_spam_path):\n",
    "    os.makedirs(train_spam_path)\n",
    "if not os.path.exists(train_ham_path):\n",
    "    os.makedirs(train_ham_path)\n",
    "if not os.path.exists(test_spam_path):\n",
    "    os.makedirs(test_spam_path)\n",
    "if not os.path.exists(test_ham_path):\n",
    "    os.makedirs(test_ham_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. File Encoding Format Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_encoding_format_conversion_save(file_in, file_out, encoding_in='gb2312', encoding_out='utf-8'):\n",
    "    with codecs.open(filename=file_in, mode='r', encoding=encoding_in, errors='replace') as fi:\n",
    "        data = fi.read()\n",
    "        with open(file_out, mode='w', encoding=encoding_out) as fo:\n",
    "            fo.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 5000 completed...\n",
      "No. 10000 completed...\n",
      "No. 15000 completed...\n",
      "No. 20000 completed...\n",
      "No. 25000 completed...\n",
      "No. 30000 completed...\n",
      "No. 35000 completed...\n",
      "No. 40000 completed...\n",
      "No. 45000 completed...\n",
      "No. 50000 completed...\n",
      "No. 55000 completed...\n",
      "No. 60000 completed...\n"
     ]
    }
   ],
   "source": [
    "train_spam_path_list = []\n",
    "train_ham_path_list = []\n",
    "test_spam_path_list = []\n",
    "test_ham_path_list = []\n",
    "\n",
    "\n",
    "label = \"./data/trec06c/full/index\"\n",
    "index = 0\n",
    "            \n",
    "for line in open(label):\n",
    "    info = line.split()\n",
    "    \n",
    "    if index <= 50000:\n",
    "        src_file_path = './data/trec06c/data/' + info[1][-8:]\n",
    "        dst_file_path = \"./data/train/\" + info[0] + \"/\" + str(index)\n",
    "        # shutil.copyfile(src_file_path, dst_file_path)\n",
    "        \n",
    "    \n",
    "        file_encoding_format_conversion_save(file_in=src_file_path, \n",
    "                                             file_out=dst_file_path, \n",
    "                                             encoding_in='gb2312', \n",
    "                                             encoding_out='utf-8')\n",
    "        \n",
    "        if info[0] == \"spam\":\n",
    "            train_spam_path_list.append(dst_file_path)\n",
    "        else:\n",
    "            train_ham_path_list.append(dst_file_path)\n",
    "            \n",
    "    else:\n",
    "        src_file_path = './data/trec06c/data/' + info[1][-8:]\n",
    "        dst_file_path = \"./data/test/\" + info[0] + \"/\" + str(index)\n",
    "        # shutil.copyfile(src_file_path, dst_file_path)\n",
    "        \n",
    "        file_encoding_format_conversion_save(file_in=src_file_path, \n",
    "                                             file_out=dst_file_path, \n",
    "                                             encoding_in='gb2312', \n",
    "                                             encoding_out='utf-8')\n",
    "        \n",
    "        if info[0] == \"spam\":\n",
    "            test_spam_path_list.append(dst_file_path)\n",
    "        else:\n",
    "            test_ham_path_list.append(dst_file_path)\n",
    "            \n",
    "    index += 1\n",
    "    if index % 5000 == 0:\n",
    "        print(\"No. {} completed...\".format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/train/spam/0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spam_path_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stop List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_list_path):\n",
    "    stop_words_list = []\n",
    "    for line in open(stop_list_path):\n",
    "        stop_words_list.append(line[:len(line)-1])\n",
    "    return stop_words_list\n",
    "\n",
    "stop_words_list = get_stop_words(stop_list_path=\"./data/stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--', '?', '“', '”', '》']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. jieba Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_word_set_of_single_email(email_path):\n",
    "    word_list = list()\n",
    "    for line in open(email_path):\n",
    "        rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(\"\", line)\n",
    "        word_list += list(jieba.cut(line))\n",
    "    \n",
    "    word_set = set()\n",
    "    for item in word_list:\n",
    "        if item not in stop_words_list and item.strip() != '' and item != None:\n",
    "            word_set.add(item)\n",
    "            \n",
    "    return word_set\n",
    "\n",
    "def get_the_dict_by_emails(email_paths_list):\n",
    "    index = 0\n",
    "    \n",
    "    all_emails_word_dict = dict()\n",
    "    all_emails_word_set = set()\n",
    "    \n",
    "    for path in tqdm(email_paths_list):\n",
    "        current_email_set = get_the_word_set_of_single_email(path)\n",
    "        all_emails_word_dict[index] = current_email_set\n",
    "        all_emails_word_set = all_emails_word_set | current_email_set\n",
    "        index += 1\n",
    "\n",
    "    return (all_emails_word_dict, all_emails_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 33262/33262 [06:09<00:00, 89.90it/s] \n"
     ]
    }
   ],
   "source": [
    "train_spam_word_dict, train_spam_word_set = get_the_dict_by_emails(train_spam_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 16739/16739 [03:08<00:00, 88.58it/s] \n"
     ]
    }
   ],
   "source": [
    "train_ham_word_dict, train_ham_word_set = get_the_dict_by_emails(train_ham_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency_dict(all_word_dict, all_word_set):\n",
    "    word_frequency_dict = dict()\n",
    "    for item in tqdm(all_word_set):\n",
    "        index = 0\n",
    "        for email_index, single_word_set in all_word_dict.items():\n",
    "            if item in single_word_set:\n",
    "                index += 1\n",
    "        word_frequency_dict[item] = index\n",
    "    return word_frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 78721/78721 [07:52<00:00, 166.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train_spam_word_frequency_dict = get_word_frequency_dict(train_spam_word_dict, train_spam_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 99681/99681 [04:04<00:00, 407.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ham_word_frequency_dict = get_word_frequency_dict(train_ham_word_dict, train_ham_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33262, 16739)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_spam_number = len(train_spam_path_list)\n",
    "train_ham_number = len(train_ham_path_list)\n",
    "train_spam_number, train_ham_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6652266954660907, 0.3347733045339093)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_prior_spam = train_spam_number / (train_spam_number + train_ham_number)\n",
    "p_prior_ham = train_ham_number / (train_spam_number + train_ham_number)\n",
    "p_prior_spam, p_prior_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_word_prob(test_email_word_set,\n",
    "                       spam_word_frequency_dict=train_spam_word_frequency_dict,\n",
    "                       ham_word_frequency_dict=train_ham_word_frequency_dict,\n",
    "                       spam_number=train_spam_number,\n",
    "                       ham_number=train_ham_number\n",
    "                      ):\n",
    "    \n",
    "    word_prob_dict = dict()\n",
    "    \n",
    "    for word in test_email_word_set:\n",
    "        if word in spam_word_frequency_dict.keys() and word in ham_word_frequency_dict.keys():\n",
    "            p_word_spam = spam_word_frequency_dict[word] / spam_number\n",
    "            p_word_ham = ham_word_frequency_dict[word] / ham_number\n",
    "            word_prob_dict.setdefault(word, (p_word_spam, p_word_ham))\n",
    "        elif word in spam_word_frequency_dict.keys() and word not in ham_word_frequency_dict.keys():\n",
    "            p_word_spam = spam_word_frequency_dict[word] / spam_number\n",
    "            p_word_ham = 0.01\n",
    "            word_prob_dict.setdefault(word, (p_word_spam, p_word_ham))\n",
    "        elif word not in spam_word_frequency_dict.keys() and word in ham_word_frequency_dict.keys():\n",
    "            p_word_spam = 0.01\n",
    "            p_word_ham = ham_word_frequency_dict[word] / ham_number\n",
    "            word_prob_dict.setdefault(word, (p_word_spam, p_word_ham))\n",
    "        elif word not in spam_word_frequency_dict.keys() and word not in ham_word_frequency_dict.keys():\n",
    "            #若该词不在脏词词典中，概率设为0.4\n",
    "            p_word_spam = 0.01\n",
    "            p_word_ham = 0.01\n",
    "            word_prob_dict.setdefault(word, (p_word_spam, p_word_ham))\n",
    "            \n",
    "    # print(sorted(word_prob_dict.items(), key=lambda d:d[1], reverse=True)[0:15])\n",
    "    return word_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_single_email_bayes(email_path, p_prior_spam=p_prior_spam, p_prior_ham=p_prior_ham):\n",
    "    test_email_word_set = get_the_word_set_of_single_email(email_path)\n",
    "    email_word_prob_dict = get_test_word_prob(test_email_word_set)\n",
    "    \n",
    "    email_word_prob_list = sorted(email_word_prob_dict.items(), key=lambda d:d[1], reverse=True)[0:15]\n",
    "    \n",
    "    p_word_spam = float(p_prior_spam) * 1e6\n",
    "    p_word_ham = float(p_prior_ham) * 1e6\n",
    "    \n",
    "    \"\"\"\n",
    "    for word, (spam_prob, ham_prob) in email_word_prob_dict.items():\n",
    "        p_word_spam *= (spam_prob)\n",
    "        p_word_ham *= (ham_prob)\n",
    "    \"\"\"\n",
    "    for word, (spam_prob, ham_prob) in email_word_prob_list:\n",
    "        p_word_spam *= (spam_prob)\n",
    "        p_word_ham *= (ham_prob)\n",
    "        \n",
    "    p = p_word_spam / (p_word_spam + p_word_ham + 1e-12)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(email_paths, label=\"spam\"):\n",
    "    \n",
    "    num = len(email_paths)\n",
    "    \n",
    "    if label == \"spam\":\n",
    "        index = True\n",
    "    else:\n",
    "        index = False\n",
    "        \n",
    "    correct_conuts = 0\n",
    "    wrong_counts = 0\n",
    "    for path in tqdm(email_paths):\n",
    "        p = get_test_single_email_bayes(path)\n",
    "        if p >= 0.9:\n",
    "            correct_conuts = correct_conuts + 1\n",
    "        else:\n",
    "            wrong_counts = wrong_counts + 1\n",
    "    \n",
    "    return (correct_conuts, wrong_counts) if index else (wrong_counts, correct_conuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 9592/9592 [01:21<00:00, 118.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8431, 1161)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc(test_spam_path_list, label=\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5027/5027 [00:39<00:00, 128.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4353, 674)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc(test_ham_path_list, label=\"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8789616346955796, 0.12103836530442035)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8431 / 9592, 1161 / 9592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8659240103441417, 0.13407598965585837)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4353 / 5027, 674 / 5027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773513920240783"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8473 + 4353) / (9592 + 5027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
